{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a8e6313",
   "metadata": {},
   "source": [
    "1. 有监督\n",
    "2. 无/自监督：对比学习或者跟自己比（Diffusion，图片匹配）\n",
    "3. 迁移学习：猫狗分类->狮子老虎分类，model不变，最终结果加NN进行处理就可以了。\n",
    "4. 对抗学习：两个模型互相学（GAN：竞争）\n",
    "5. 强化学习：从环境里面学（agent产生动作，环境规则产生反馈reward，使用模型作为评论家给出评价。协同关系。）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78ea7a6",
   "metadata": {},
   "source": [
    "框架：Encoder-Decoder，GAN，Diffusion\n",
    "\n",
    "方法：CNN，Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048e4a62",
   "metadata": {},
   "source": [
    "模型选择：baseline研究是重点，防止闭门造车。\n",
    "\n",
    "1. 优化器和批次：通用优化器优先。如果模型变深则优化器也应该调整，比如增强正则化或额外的正则化技术。\n",
    "    由于样本方差存在，批次较小会带来噪声，类似正则化效果，比较不容易过拟合。\n",
    "    批次大小不应该作为参数，虽然会有影响，但是轮数达到一定次数会抹平这个差距。\n",
    "2. 炼丹大法：\n",
    "    1) 增量式炼丹：先从简单的配置开始，慢慢修改，发现有效方法。\n",
    "        > 参数空间刚开始考虑使用原模型的参数，然后增加新的功能或调节。可能在模型更改后需要重新设计参数，进行多次调整。\n",
    "\n",
    "        > 当我们得到较好结果，需要保证改动有合理的证据！并且考虑是否选择这次的结果。\n",
    "\n",
    "        > 需要在调参过程中确定哪些超参数对结果影响较大，比较敏感；哪些参数间存在相互作用；那些超参数对变化不敏感可以固定下来。\n",
    "\n",
    "        > 当调参空间缩小到一定程度后，就可以进行大量实验了。\n",
    "    2) 优先理解问题，这会花费大量时间。然后才是精确调整。\n",
    "        > 每一轮需要有一个目标，控制变量。并且参数可以分三类：核心参数（如层数、dropout，dropout率则是干扰参数；）；干扰参数（每次修改核心参数，都需要重新修改干扰参数，如优化器的学习率参数之类的，但是优化器类别一般不修改）；固定参数被认为影响较小的参数（激活函数）。\n",
    "        > 但是超参数地位也是会变化的。\n",
    "\n",
    "        > ![image.png](02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3357467",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
